{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк датасета для классификации:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      "Размер датасета: (1599, 12)\n"
     ]
    }
   ],
   "source": [
    "path = 'winequality-red.csv'\n",
    "data_class = pd.read_csv(path)\n",
    "\n",
    "print(\"Первые 5 строк датасета для классификации:\")\n",
    "print(data_class.head())\n",
    "print(f\"\\nРазмер датасета: {data_class.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем целевую переменную для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение качества вина:\n",
      "wine_quality\n",
      "good    855\n",
      "bad     744\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_class['wine_quality'] = data_class['quality'].apply(lambda x: 'good' if x >= 6 else 'bad')\n",
    "data_class = data_class.drop('quality', axis=1)\n",
    "\n",
    "print(\"\\nРаспределение качества вина:\")\n",
    "print(data_class['wine_quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим датасет на признаки и целевую переменную, масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (1279, 11), Размер тестовой выборки: (320, 11)\n"
     ]
    }
   ],
   "source": [
    "X = data_class.drop('wine_quality', axis=1)\n",
    "y = data_class['wine_quality']\n",
    "\n",
    "# масштабирование \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применим встроенное решающее дерево для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (решающее дерево):\n",
      "Accuracy: 0.7562\n",
      "F1-Score: 0.7561\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.74      0.73      0.74       149\n",
      "        good       0.77      0.78      0.77       171\n",
      "\n",
      "    accuracy                           0.76       320\n",
      "   macro avg       0.76      0.75      0.75       320\n",
      "weighted avg       0.76      0.76      0.76       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_baseline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Бейзлайн (решающее дерево):\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подберем гиперпараметры для оптимизации бейзлайна\n",
    "\n",
    "##### `max_depth: range(3, 21)`\n",
    "- **Контроль сложности сверху** - ограничивает максимальную глубину дерева\n",
    "- **Диапазон 3-20**: от простых интерпретируемых деревьев до сложных моделей\n",
    "- **Малые значения (3-8)**: предотвращают переобучение, легко интерпретировать\n",
    "- **Средние значения (8-15)**: баланс между точностью и сложностью\n",
    "- **Большие значения (15-20)**: риск переобучения, но могут выявлять сложные паттерны\n",
    "\n",
    "##### `min_samples_split: range(2, 10)`\n",
    "- **Контроль разделения узлов** - минимальное samples для разделения\n",
    "- **Диапазон 2-9**: от максимально детализированных до устойчивых деревьев\n",
    "- **Малые значения (2-4)**: тонкое разделение, риск переобучения на шуме\n",
    "- **Средние значения (4-7)**: баланс между детализацией и обобщением\n",
    "- **Большие значения (7-9)**: устойчивость к шуму, риск недообучения\n",
    "\n",
    "##### `min_samples_leaf: range(1, 10)`\n",
    "- **Контроль конечных узлов** - минимальное samples в листьях\n",
    "- **Диапазон 1-9**: предотвращает создание листов с малым количеством samples\n",
    "- **Малые значения (1-3)**: детализированные листы, чувствительность к шуму\n",
    "- **Средние значения (3-6)**: стабильные предсказания\n",
    "- **Большие значения (6-9)**: сглаженные предсказания, устойчивость к выбросам\n",
    "\n",
    "##### Общая стратегия\n",
    "Комплексный контроль сложности дерева через ограничение глубины, условий разделения и размера конечных узлов для баланса между точностью и переобучением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Лучшие параметры: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': range(3, 21),\n",
    "    'min_samples_split': range(2, 10),\n",
    "    'min_samples_leaf': range(1, 10)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Лучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модель с этими гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (решающее дерево):\n",
      "Accuracy: 0.7594\n",
      "F1-Score: 0.7589\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.75      0.72      0.74       149\n",
      "        good       0.76      0.80      0.78       171\n",
      "\n",
      "    accuracy                           0.76       320\n",
      "   macro avg       0.76      0.76      0.76       320\n",
      "weighted avg       0.76      0.76      0.76       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted')\n",
    "\n",
    "print(\"Улучшенный бейзлайн (решающее дерево):\")\n",
    "print(f\"Accuracy: {accuracy_best:.4f}\")\n",
    "print(f\"F1-Score: {f1_best:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Собственная имплементация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Реализация Custom Decision Tree:\n",
      "Accuracy: 0.7188\n",
      "F1-Score: 0.7188\n",
      "Custom Decision Tree с улучшенными параметрами:\n",
      "Accuracy: 0.7531\n",
      "F1-Score: 0.7528\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Добавляем проверку min_samples_leaf\n",
    "        if (len(unique_labels) == 1 or \n",
    "            depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or\n",
    "            n_samples < 2 * self.min_samples_leaf):  # Нельзя разделить если samples < 2*min_samples_leaf\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        \n",
    "        # Проверяем min_samples_leaf для дочерних узлов\n",
    "        if sum(left_indices) < self.min_samples_leaf or sum(right_indices) < self.min_samples_leaf:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        left_child = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature_idx], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_column, threshold):\n",
    "        parent_loss = self._gini(y)\n",
    "\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "        if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = sum(left_indices), sum(right_indices)\n",
    "        e_left, e_right = self._gini(y[left_indices]), self._gini(y[right_indices])\n",
    "\n",
    "        child_loss = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _gini(self, y):\n",
    "        proportions = [np.sum(y == c) / len(y) for c in np.unique(y)]\n",
    "        return 1 - sum([p ** 2 for p in proportions])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "custom_dt = CustomDecisionTree()\n",
    "custom_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom = custom_dt.predict(X_test)\n",
    "\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "f1_custom = f1_score(y_test, y_pred_custom, average='weighted')\n",
    "\n",
    "print(\"Реализация Custom Decision Tree:\")\n",
    "print(f\"Accuracy: {accuracy_custom:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom:.4f}\")\n",
    "\n",
    "custom_dt_improved = CustomDecisionTree(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=1  # можно тоже подбирать\n",
    ")\n",
    "custom_dt_improved.fit(X_train, y_train)\n",
    "y_pred_custom_improved = custom_dt_improved.predict(X_test)\n",
    "\n",
    "accuracy_custom_improved = accuracy_score(y_test, y_pred_custom_improved)\n",
    "f1_custom_improved = f1_score(y_test, y_pred_custom_improved, average='weighted')\n",
    "\n",
    "print(\"Custom Decision Tree с улучшенными параметрами:\")\n",
    "print(f\"Accuracy: {accuracy_custom_improved:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи классификации:\n",
      "Бейзлайн Accuracy: 0.7562, Улучшенный Accuracy: 0.7594, Custom Decision Tree Accuracy: 0.7188, Улучшенный Custom Decision Tree Accuracy: 0.7531\n",
      "Бейзлайн F1-Score: 0.7561, Улучшенный F1-Score: 0.7589, Custom Decision Tree F1-Score: 0.7188, Улучшенный Custom Decision Tree F1-Score: 0.7528\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи классификации:\")\n",
    "print(f\"Бейзлайн Accuracy: {accuracy:.4f}, Улучшенный Accuracy: {accuracy_best:.4f}, Custom Decision Tree Accuracy: {accuracy_custom:.4f}, Улучшенный Custom Decision Tree Accuracy: {accuracy_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн F1-Score: {f1:.4f}, Улучшенный F1-Score: {f1_best:.4f}, Custom Decision Tree F1-Score: {f1_custom:.4f}, Улучшенный Custom Decision Tree F1-Score: {f1_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые 5 строк датасета для регрессии:\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "Используемые признаки: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
      "Размер X: (17379, 12)\n"
     ]
    }
   ],
   "source": [
    "path = 'hour.csv'\n",
    "data_reg = pd.read_csv(path)\n",
    "\n",
    "print(\"\\nПервые 5 строк датасета для регрессии:\")\n",
    "print(data_reg.head())\n",
    "\n",
    "# убрал дату в строковом представлении (есть дата в числовых колонках), так же число, которое предсказываем (cnt)\n",
    "useful_columns = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', \n",
    "                  'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "# Разделим данные на тестовую и обучающую выборку\n",
    "X = data_reg[useful_columns]\n",
    "y = data_reg['cnt']\n",
    "\n",
    "print(f\"\\nИспользуемые признаки: {list(X.columns)}\")\n",
    "print(f\"Размер X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (13903, 12), Размер тестовой выборки: (3476, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_reg = StandardScaler()\n",
    "X_train_scaled = scaler_reg.fit_transform(X_train)\n",
    "X_test_scaled = scaler_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение бейзлайна и оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Бейзлайн (Decision Tree Regressor):\n",
      "MAE: 34.1997, MSE: 3405.1715, R^2: 0.8925\n"
     ]
    }
   ],
   "source": [
    "dt_baseline = DecisionTreeRegressor(random_state=42)\n",
    "dt_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_reg = dt_baseline.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "print(\"\\nБейзлайн (Decision Tree Regressor):\")\n",
    "mae_baseline, mse_baseline, r2_baseline = evaluate_model(y_test, y_pred_reg)\n",
    "print(f\"MAE: {mae_baseline:.4f}, MSE: {mse_baseline:.4f}, R^2: {r2_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подберем гиперпараметры для улучшения модели\n",
    "\n",
    "##### `max_depth: range(1, 20)`\n",
    "- **Расширенный диапазон 1-19**: включает более простые деревья (глубиной 1-2)\n",
    "- **Очень малые значения (1-3)**: \"пни\" и простые деревья для базовых закономерностей\n",
    "- **Полный спектр**: от максимально простых до сложных моделей\n",
    "\n",
    "##### `min_samples_split: range(2, 10)`\n",
    "- **Стандартный диапазон 2-9**: контроль минимального размера для разделения узлов\n",
    "- **Охватывает** от максимально детализированных до устойчивых разделений\n",
    "\n",
    "##### `min_samples_leaf: range(1, 10)`\n",
    "- **Контроль размера листьев**: предотвращает создание листов с малым количеством samples\n",
    "- **Диапазон 1-9**: от отдельных наблюдений до сгруппированных предсказаний\n",
    "- **Стабилизация предсказаний**: особенно важно для несбалансированных данных\n",
    "\n",
    "##### `max_features: [None, 'sqrt', 'log2']`\n",
    "- **Случайный выбор признаков**: ключевой параметр для ансамблевых методов\n",
    "- **None**: использование всех признаков (стандартное поведение)\n",
    "- **'sqrt'**: sqrt(n_features) - стандартный выбор для случайного леса\n",
    "- **'log2'**: log_2(n_features) - альтернатива для данных с многими признаками\n",
    "- **Разнообразие деревьев**: особенно важно при переходе к ансамблевым методам\n",
    "\n",
    "##### Общая стратегия\n",
    "Комплексный подход, включающий как стандартные параметры контроля сложности, так и параметры для увеличения разнообразия, что особенно ценно при построении ансамблей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "\n",
      "Лучшие параметры: {'max_depth': 16, 'min_samples_leaf': 3, 'min_samples_split': 9}\n"
     ]
    }
   ],
   "source": [
    "param_grid_extended = {\n",
    "    'max_depth': range(1, 20),\n",
    "    'min_samples_split': range(2, 10),\n",
    "    'min_samples_leaf': range(1, 10),      # Минимальные samples в листе\n",
    "    'max_features': [None, 'sqrt', 'log2'] # Количество признаков для разделения\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    verbose=1,\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nЛучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели с подобранными гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Улучшенная модель (Decision Tree Regressor):\n",
      "MAE: 30.9878, MSE: 2845.4693, R^2: 0.9101\n"
     ]
    }
   ],
   "source": [
    "dt_optimized = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "dt_optimized.fit(X_train_scaled, y_train)\n",
    "y_pred_optimized = dt_optimized.predict(X_test_scaled)\n",
    "\n",
    "mae_optimized, mse_optimized, r2_optimized = evaluate_model(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"\\nУлучшенная модель (Decision Tree Regressor):\")\n",
    "print(f\"MAE: {mae_optimized:.4f}, MSE: {mse_optimized:.4f}, R^2: {r2_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуем собственную имплементацию алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Decision Tree Regressor:\n",
      "MAE: 70.2262, MSE: 11151.1214, R^2: 0.6478\n",
      "Custom Decision Tree Regressor с улучшенными параметрами:\n",
      "MAE: 31.3224, MSE: 2907.2484, R²: 0.9082\n"
     ]
    }
   ],
   "source": [
    "class CustomDecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(self.tree, x) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if (depth >= self.max_depth or \n",
    "            len(np.unique(y)) == 1 or \n",
    "            len(y) < self.min_samples_split):\n",
    "            return np.mean(y)\n",
    "\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_mask = X[:, best_feature] < best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Проверяем минимальное количество samples\n",
    "        if sum(left_mask) < 1 or sum(right_mask) < 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_child, \"right\": right_child}\n",
    "\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_mse = None, None, float(\"inf\")\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "                    continue\n",
    "\n",
    "                mse = (\n",
    "                    len(y[left_mask]) * np.var(y[left_mask]) +\n",
    "                    len(y[right_mask]) * np.var(y[right_mask])\n",
    "                )\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _predict_sample(self, tree, x):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree[\"feature\"]] < tree[\"threshold\"]:\n",
    "            return self._predict_sample(tree[\"left\"], x)\n",
    "        else:\n",
    "            return self._predict_sample(tree[\"right\"], x)\n",
    "\n",
    "custom_dt_reg = CustomDecisionTreeRegressor()\n",
    "custom_dt_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg = custom_dt_reg.predict(X_test_scaled)\n",
    "\n",
    "mae_custom, mse_custom, r2_custom = evaluate_model(y_test, y_pred_custom_reg)\n",
    "\n",
    "print(\"\\nCustom Decision Tree Regressor:\")\n",
    "print(f\"MAE: {mae_custom:.4f}, MSE: {mse_custom:.4f}, R^2: {r2_custom:.4f}\")\n",
    "\n",
    "custom_dt_reg_improved = CustomDecisionTreeRegressor(\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"]\n",
    ")\n",
    "custom_dt_reg_improved.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg_improved = custom_dt_reg_improved.predict(X_test_scaled)\n",
    "\n",
    "mae_custom_improved, mse_custom_improved, r2_custom_improved = evaluate_model(y_test, y_pred_custom_reg_improved)\n",
    "\n",
    "print(\"Custom Decision Tree Regressor с улучшенными параметрами:\")\n",
    "print(f\"MAE: {mae_custom_improved:.4f}, MSE: {mse_custom_improved:.4f}, R²: {r2_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравним полученные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи регрессии:\n",
      "Бейзлайн MAE: 34.1997, Улучшенный MAE: 30.9878, Custom MAE: 70.2262, Улучшенный Custom MAE: 31.3224\n",
      "Бейзлайн MSE: 3405.1715, Улучшенный MSE: 2845.4693, Custom MSE: 11151.1214, Улучшенный Custom MSE: 2907.2484\n",
      "Бейзлайн R^2: 0.8925, Улучшенный R^2: 0.9101, Custom R^2: 0.6478 , Улучшенный Custom R^2: 0.9082\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи регрессии:\")\n",
    "print(f\"Бейзлайн MAE: {mae_baseline:.4f}, Улучшенный MAE: {mae_optimized:.4f}, Custom MAE: {mae_custom:.4f}, Улучшенный Custom MAE: {mae_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн MSE: {mse_baseline:.4f}, Улучшенный MSE: {mse_optimized:.4f}, Custom MSE: {mse_custom:.4f}, Улучшенный Custom MSE: {mse_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн R^2: {r2_baseline:.4f}, Улучшенный R^2: {r2_optimized:.4f}, Custom R^2: {r2_custom:.4f} , Улучшенный Custom R^2: {r2_custom_improved:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

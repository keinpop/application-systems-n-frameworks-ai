{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк датасета для классификации:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      "Размер датасета: (1599, 12)\n"
     ]
    }
   ],
   "source": [
    "path = 'winequality-red.csv'\n",
    "data_class = pd.read_csv(path)\n",
    "\n",
    "print(\"Первые 5 строк датасета для классификации:\")\n",
    "print(data_class.head())\n",
    "print(f\"\\nРазмер датасета: {data_class.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем целевую переменную для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение качества вина:\n",
      "wine_quality\n",
      "good    855\n",
      "bad     744\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_class['wine_quality'] = data_class['quality'].apply(lambda x: 'good' if x >= 6 else 'bad')\n",
    "data_class = data_class.drop('quality', axis=1)\n",
    "\n",
    "print(\"\\nРаспределение качества вина:\")\n",
    "print(data_class['wine_quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим датасет на признаки и целевую переменную, масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (1279, 11), Размер тестовой выборки: (320, 11)\n"
     ]
    }
   ],
   "source": [
    "X = data_class.drop('wine_quality', axis=1)\n",
    "y = data_class['wine_quality']\n",
    "\n",
    "# масштабирование \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применим встроенное решающее дерево для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (решающее дерево):\n",
      "Accuracy: 0.7562\n",
      "F1-Score: 0.7561\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.74      0.73      0.74       149\n",
      "        good       0.77      0.78      0.77       171\n",
      "\n",
      "    accuracy                           0.76       320\n",
      "   macro avg       0.76      0.75      0.75       320\n",
      "weighted avg       0.76      0.76      0.76       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_baseline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Бейзлайн (решающее дерево):\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подберем гиперпараметры для оптимизации бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Лучшие параметры: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': range(3, 21),\n",
    "    'min_samples_split': range(2, 10),\n",
    "    'min_samples_leaf': range(1, 10)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Лучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модель с этими гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (решающее дерево):\n",
      "Accuracy: 0.7594\n",
      "F1-Score: 0.7589\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.75      0.72      0.74       149\n",
      "        good       0.76      0.80      0.78       171\n",
      "\n",
      "    accuracy                           0.76       320\n",
      "   macro avg       0.76      0.76      0.76       320\n",
      "weighted avg       0.76      0.76      0.76       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted')\n",
    "\n",
    "print(\"Улучшенный бейзлайн (решающее дерево):\")\n",
    "print(f\"Accuracy: {accuracy_best:.4f}\")\n",
    "print(f\"F1-Score: {f1_best:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Собственная имплементация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Реализация Custom Decision Tree:\n",
      "Accuracy: 0.7188\n",
      "F1-Score: 0.7188\n",
      "Custom Decision Tree с улучшенными параметрами:\n",
      "Accuracy: 0.7531\n",
      "F1-Score: 0.7528\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Добавляем проверку min_samples_leaf\n",
    "        if (len(unique_labels) == 1 or \n",
    "            depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or\n",
    "            n_samples < 2 * self.min_samples_leaf):  # Нельзя разделить если samples < 2*min_samples_leaf\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "        \n",
    "        # Проверяем min_samples_leaf для дочерних узлов\n",
    "        if sum(left_indices) < self.min_samples_leaf or sum(right_indices) < self.min_samples_leaf:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=most_common_label)\n",
    "\n",
    "        left_child = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature_idx], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_column, threshold):\n",
    "        parent_loss = self._gini(y)\n",
    "\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = feature_column > threshold\n",
    "        if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = sum(left_indices), sum(right_indices)\n",
    "        e_left, e_right = self._gini(y[left_indices]), self._gini(y[right_indices])\n",
    "\n",
    "        child_loss = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _gini(self, y):\n",
    "        proportions = [np.sum(y == c) / len(y) for c in np.unique(y)]\n",
    "        return 1 - sum([p ** 2 for p in proportions])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "custom_dt = CustomDecisionTree()\n",
    "custom_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom = custom_dt.predict(X_test)\n",
    "\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "f1_custom = f1_score(y_test, y_pred_custom, average='weighted')\n",
    "\n",
    "print(\"Реализация Custom Decision Tree:\")\n",
    "print(f\"Accuracy: {accuracy_custom:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom:.4f}\")\n",
    "\n",
    "custom_dt_improved = CustomDecisionTree(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=1  # можно тоже подбирать\n",
    ")\n",
    "custom_dt_improved.fit(X_train, y_train)\n",
    "y_pred_custom_improved = custom_dt_improved.predict(X_test)\n",
    "\n",
    "accuracy_custom_improved = accuracy_score(y_test, y_pred_custom_improved)\n",
    "f1_custom_improved = f1_score(y_test, y_pred_custom_improved, average='weighted')\n",
    "\n",
    "print(\"Custom Decision Tree с улучшенными параметрами:\")\n",
    "print(f\"Accuracy: {accuracy_custom_improved:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи классификации:\n",
      "Бейзлайн Accuracy: 0.7562, Улучшенный Accuracy: 0.7594, Custom Decision Tree Accuracy: 0.7188, Улучшенный Custom Decision Tree Accuracy: 0.7531\n",
      "Бейзлайн F1-Score: 0.7561, Улучшенный F1-Score: 0.7589, Custom Decision Tree F1-Score: 0.7188, Улучшенный Custom Decision Tree F1-Score: 0.7528\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи классификации:\")\n",
    "print(f\"Бейзлайн Accuracy: {accuracy:.4f}, Улучшенный Accuracy: {accuracy_best:.4f}, Custom Decision Tree Accuracy: {accuracy_custom:.4f}, Улучшенный Custom Decision Tree Accuracy: {accuracy_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн F1-Score: {f1:.4f}, Улучшенный F1-Score: {f1_best:.4f}, Custom Decision Tree F1-Score: {f1_custom:.4f}, Улучшенный Custom Decision Tree F1-Score: {f1_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые 5 строк датасета для регрессии:\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "Используемые признаки: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered']\n",
      "Размер X: (17379, 13)\n"
     ]
    }
   ],
   "source": [
    "path = 'hour.csv'\n",
    "data_reg = pd.read_csv(path)\n",
    "\n",
    "print(\"\\nПервые 5 строк датасета для регрессии:\")\n",
    "print(data_reg.head())\n",
    "\n",
    "# убрал дату в строковом представлении (есть дата в числовых колонках), так же число, которое предсказываем (cnt)\n",
    "useful_columns = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', \n",
    "                  'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered']\n",
    "\n",
    "# Разделим данные на тестовую и обучающую выборку\n",
    "X = data_reg[useful_columns]\n",
    "y = data_reg['cnt']\n",
    "\n",
    "print(f\"\\nИспользуемые признаки: {list(X.columns)}\")\n",
    "print(f\"Размер X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (13903, 13), Размер тестовой выборки: (3476, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_reg = StandardScaler()\n",
    "X_train_scaled = scaler_reg.fit_transform(X_train)\n",
    "X_test_scaled = scaler_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение бейзлайна и оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Бейзлайн (Decision Tree Regressor):\n",
      "MAE: 13.4833, MSE: 587.1974, R^2: 0.9815\n"
     ]
    }
   ],
   "source": [
    "dt_baseline = DecisionTreeRegressor(random_state=42)\n",
    "dt_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_reg = dt_baseline.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "print(\"\\nБейзлайн (Decision Tree Regressor):\")\n",
    "mae_baseline, mse_baseline, r2_baseline = evaluate_model(y_test, y_pred_reg)\n",
    "print(f\"MAE: {mae_baseline:.4f}, MSE: {mse_baseline:.4f}, R^2: {r2_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подберем гиперпараметры для улучшения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "\n",
      "Лучшие параметры: {'max_depth': 15, 'min_samples_leaf': 8, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid_extended = {\n",
    "    'max_depth': range(1, 20),\n",
    "    'min_samples_split': range(2, 10),\n",
    "    'min_samples_leaf': range(1, 10),      # Минимальные samples в листе\n",
    "    'max_features': [None, 'sqrt', 'log2'] # Количество признаков для разделения\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    verbose=1,\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nЛучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели с подобранными гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Улучшенная модель (Decision Tree Regressor):\n",
      "MAE: 11.8963, MSE: 454.4274, R^2: 0.9856\n"
     ]
    }
   ],
   "source": [
    "dt_optimized = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "dt_optimized.fit(X_train_scaled, y_train)\n",
    "y_pred_optimized = dt_optimized.predict(X_test_scaled)\n",
    "\n",
    "mae_optimized, mse_optimized, r2_optimized = evaluate_model(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"\\nУлучшенная модель (Decision Tree Regressor):\")\n",
    "print(f\"MAE: {mae_optimized:.4f}, MSE: {mse_optimized:.4f}, R^2: {r2_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуем собственную имплементацию алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Decision Tree Regressor:\n",
      "MAE: 19.3083, MSE: 944.8642, R^2: 0.9702\n",
      "Custom Decision Tree Regressor с улучшенными параметрами:\n",
      "MAE: 13.1441, MSE: 558.5635, R²: 0.9824\n"
     ]
    }
   ],
   "source": [
    "class CustomDecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(self.tree, x) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if (depth >= self.max_depth or \n",
    "            len(np.unique(y)) == 1 or \n",
    "            len(y) < self.min_samples_split):\n",
    "            return np.mean(y)\n",
    "\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_mask = X[:, best_feature] < best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Проверяем минимальное количество samples\n",
    "        if sum(left_mask) < 1 or sum(right_mask) < 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_child, \"right\": right_child}\n",
    "\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_mse = None, None, float(\"inf\")\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "                    continue\n",
    "\n",
    "                mse = (\n",
    "                    len(y[left_mask]) * np.var(y[left_mask]) +\n",
    "                    len(y[right_mask]) * np.var(y[right_mask])\n",
    "                )\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _predict_sample(self, tree, x):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree[\"feature\"]] < tree[\"threshold\"]:\n",
    "            return self._predict_sample(tree[\"left\"], x)\n",
    "        else:\n",
    "            return self._predict_sample(tree[\"right\"], x)\n",
    "\n",
    "custom_dt_reg = CustomDecisionTreeRegressor()\n",
    "custom_dt_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg = custom_dt_reg.predict(X_test_scaled)\n",
    "\n",
    "mae_custom, mse_custom, r2_custom = evaluate_model(y_test, y_pred_custom_reg)\n",
    "\n",
    "print(\"\\nCustom Decision Tree Regressor:\")\n",
    "print(f\"MAE: {mae_custom:.4f}, MSE: {mse_custom:.4f}, R^2: {r2_custom:.4f}\")\n",
    "\n",
    "custom_dt_reg_improved = CustomDecisionTreeRegressor(\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"]\n",
    ")\n",
    "custom_dt_reg_improved.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg_improved = custom_dt_reg_improved.predict(X_test_scaled)\n",
    "\n",
    "mae_custom_improved, mse_custom_improved, r2_custom_improved = evaluate_model(y_test, y_pred_custom_reg_improved)\n",
    "\n",
    "print(\"Custom Decision Tree Regressor с улучшенными параметрами:\")\n",
    "print(f\"MAE: {mae_custom_improved:.4f}, MSE: {mse_custom_improved:.4f}, R²: {r2_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравним полученные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи регрессии:\n",
      "Бейзлайн MAE: 13.4833, Улучшенный MAE: 11.8963, Custom MAE: 19.3083, Улучшенный Custom MAE: 13.1441\n",
      "Бейзлайн MSE: 587.1974, Улучшенный MSE: 454.4274, Custom MSE: 944.8642, Улучшенный Custom MSE: 558.5635\n",
      "Бейзлайн R^2: 0.9815, Улучшенный R^2: 0.9856, Custom R^2: 0.9702 , Улучшенный Custom R^2: 0.9824\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи регрессии:\")\n",
    "print(f\"Бейзлайн MAE: {mae_baseline:.4f}, Улучшенный MAE: {mae_optimized:.4f}, Custom MAE: {mae_custom:.4f}, Улучшенный Custom MAE: {mae_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн MSE: {mse_baseline:.4f}, Улучшенный MSE: {mse_optimized:.4f}, Custom MSE: {mse_custom:.4f}, Улучшенный Custom MSE: {mse_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн R^2: {r2_baseline:.4f}, Улучшенный R^2: {r2_optimized:.4f}, Custom R^2: {r2_custom:.4f} , Улучшенный Custom R^2: {r2_custom_improved:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

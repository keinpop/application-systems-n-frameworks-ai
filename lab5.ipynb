{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Задача классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк датасета для классификации:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      "Размер датасета: (1599, 12)\n"
     ]
    }
   ],
   "source": [
    "path = 'winequality-red.csv'\n",
    "data_class = pd.read_csv(path)\n",
    "\n",
    "print(\"Первые 5 строк датасета для классификации:\")\n",
    "print(data_class.head())\n",
    "print(f\"\\nРазмер датасета: {data_class.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем целевую переменную для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение качества вина:\n",
      "wine_quality\n",
      "good    855\n",
      "bad     744\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_class['wine_quality'] = data_class['quality'].apply(lambda x: 'good' if x >= 6 else 'bad')\n",
    "data_class = data_class.drop('quality', axis=1)\n",
    "\n",
    "print(\"\\nРаспределение качества вина:\")\n",
    "print(data_class['wine_quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим датасет на признаки и целевую переменную и масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (1279, 11), Размер тестовой выборки: (320, 11)\n"
     ]
    }
   ],
   "source": [
    "X = data_class.drop('wine_quality', axis=1)\n",
    "y = data_class['wine_quality']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим встроенный алгоритм для градиентного бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (Gradient Boosting):\n",
      "Accuracy: 0.7906\n",
      "F1-Score: 0.7909\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.75      0.82      0.78       149\n",
      "        good       0.83      0.77      0.80       171\n",
      "\n",
      "    accuracy                           0.79       320\n",
      "   macro avg       0.79      0.79      0.79       320\n",
      "weighted avg       0.79      0.79      0.79       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_baseline = GradientBoostingClassifier(random_state=42)\n",
    "gb_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = gb_baseline.predict(X_test)\n",
    "\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "f1_baseline = f1_score(y_test, y_pred_baseline, average='weighted')\n",
    "\n",
    "print(\"Бейзлайн (Gradient Boosting):\")\n",
    "print(f\"Accuracy: {accuracy_baseline:.4f}\")\n",
    "print(f\"F1-Score: {f1_baseline:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подберем гиперпараметры для улучшения бейзлайна\n",
    "\n",
    "##### `n_estimators: [50, 100, 200]`\n",
    "- **Количество деревьев в ансамбле** - баланс между точностью и временем обучения\n",
    "- **50**: быстрая тренировка, подходит для начальной оценки\n",
    "- **100**: стандартный выбор, хороший баланс производительности\n",
    "- **200**: больше деревьев для потенциально лучшей точности (риск переобучения)\n",
    "\n",
    "##### `learning_rate: [0.01, 0.1, 0.2]`\n",
    "- **Темп обучения** - определяет вклад каждого дерева\n",
    "- **0.01**: медленное обучение, требует больше деревьев, но более точное\n",
    "- **0.1**: стандартное значение, хороший баланс скорости и точности\n",
    "- **0.2**: более агрессивное обучение, быстрее сходится\n",
    "\n",
    "##### `max_depth: [3, 5, 7]`\n",
    "- **Глубина деревьев** - контроль сложности weak learners\n",
    "- **3**: очень простые деревья (пни), высокая bias\n",
    "- **5**: умеренная сложность, стандартный выбор\n",
    "- **7**: более сложные деревья, лучше захватывают паттерны (риск переобучения)\n",
    "\n",
    "##### `subsample: [0.8, 1.0]`\n",
    "- **Доля samples для каждого дерева** - Стохастический градиентный бустинг\n",
    "- **0.8**: 80% данных для каждого дерева, увеличивает разнообразие\n",
    "- **1.0**: все данные (стандартный бустинг)\n",
    "\n",
    "##### `min_samples_split: [2, 5, 10]`\n",
    "- **Минимальные samples для разделения узлов**\n",
    "- **2**: максимальная детализация\n",
    "- **5**: умеренное ограничение\n",
    "- **10**: сильное ограничение, предотвращает переобучение\n",
    "\n",
    "##### Ключевые взаимодействия параметров:\n",
    "- **n_estimators × learning_rate**: компромисс - меньший learning_rate требует больше деревьев\n",
    "- **max_depth × learning_rate**: более глубокие деревья с малым learning_rate\n",
    "- **subsample**: добавляет случайность, как в Random Forest\n",
    "\n",
    "##### Общая стратегия:\n",
    "Поиск оптимального баланса между силой отдельных деревьев (max_depth) и общим ансамблем (n_estimators, learning_rate) с регуляризацией через subsampling и ограничения разделения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Лучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем обучение улучшенной модели и посчитаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (Gradient Boosting):\n",
      "Accuracy: 0.8281\n",
      "F1-Score: 0.8283\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.79      0.86      0.82       149\n",
      "        good       0.87      0.80      0.83       171\n",
      "\n",
      "    accuracy                           0.83       320\n",
      "   macro avg       0.83      0.83      0.83       320\n",
      "weighted avg       0.83      0.83      0.83       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_best = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = gb_best.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted')\n",
    "\n",
    "print(\"Улучшенный бейзлайн (Gradient Boosting):\")\n",
    "print(f\"Accuracy: {accuracy_best:.4f}\")\n",
    "print(f\"F1-Score: {f1_best:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем свой алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGradientBoosting:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.models = []\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        y_encoded = np.array([self.classes.tolist().index(label) for label in y])\n",
    "        n_classes = len(self.classes)\n",
    "        self.models = []\n",
    "        F_m = np.zeros((X.shape[0], n_classes))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            gradient = np.zeros((X.shape[0], n_classes))\n",
    "            for k in range(n_classes):\n",
    "                indicator = (y_encoded == k).astype(int)\n",
    "                probability = self._softmax(F_m[:, k])\n",
    "                gradient[:, k] = indicator - probability\n",
    "\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                random_state=np.random.randint(0, 10000)\n",
    "            )\n",
    "            tree.fit(X, gradient.argmax(axis=1))\n",
    "            self.models.append(tree)\n",
    "\n",
    "            for k in range(n_classes):\n",
    "                F_m[:, k] += self.learning_rate * tree.predict_proba(X)[:, k]\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        F_m = np.zeros((X.shape[0], len(self.classes)))\n",
    "\n",
    "        for tree in self.models:\n",
    "            tree_probs = tree.predict_proba(X)\n",
    "            for k in range(len(self.classes)):\n",
    "                F_m[:, k] += self.learning_rate * tree_probs[:, k]\n",
    "\n",
    "        y_pred_indices = np.argmax(F_m, axis=1)\n",
    "        return np.array([self.classes[i] for i in y_pred_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Gradient Boosting (базовый):\n",
      "Accuracy: 0.7063\n",
      "F1-Score: 0.7006\n",
      "Custom Gradient Boosting (улучшенный):\n",
      "Accuracy: 0.7250\n",
      "F1-Score: 0.7253\n"
     ]
    }
   ],
   "source": [
    "custom_gb = CustomGradientBoosting(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "custom_gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom_gb = custom_gb.predict(X_test)\n",
    "\n",
    "accuracy_custom_gb = accuracy_score(y_test, y_pred_custom_gb)\n",
    "f1_custom_gb = f1_score(y_test, y_pred_custom_gb, average='weighted')\n",
    "\n",
    "print(\"Custom Gradient Boosting (базовый):\")\n",
    "print(f\"Accuracy: {accuracy_custom_gb:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom_gb:.4f}\")\n",
    "\n",
    "custom_gb_improved = CustomGradientBoosting(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "custom_gb_improved.fit(X_train, y_train)\n",
    "\n",
    "y_pred_custom_gb_improved = custom_gb_improved.predict(X_test)\n",
    "\n",
    "accuracy_custom_gb_improved = accuracy_score(y_test, y_pred_custom_gb_improved)\n",
    "f1_custom_gb_improved = f1_score(y_test, y_pred_custom_gb_improved, average='weighted')\n",
    "\n",
    "print(\"Custom Gradient Boosting (улучшенный):\")\n",
    "print(f\"Accuracy: {accuracy_custom_gb_improved:.4f}\")\n",
    "print(f\"F1-Score: {f1_custom_gb_improved:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравним результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи классификации:\n",
      "Бейзлайн Accuracy: 0.7906, Улучшенный Accuracy: 0.8281, Custom GB Accuracy: 0.7063, Custom GB улучшенный Accuracy: 0.7250\n",
      "Бейзлайн F1-Score: 0.7909, Улучшенный F1-Score: 0.8283, Custom GB F1-Score: 0.7006, Custom GB улучшенный F1-Score: 0.7253\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи классификации:\")\n",
    "print(f\"Бейзлайн Accuracy: {accuracy_baseline:.4f}, Улучшенный Accuracy: {accuracy_best:.4f}, Custom GB Accuracy: {accuracy_custom_gb:.4f}, Custom GB улучшенный Accuracy: {accuracy_custom_gb_improved:.4f}\")\n",
    "print(f\"Бейзлайн F1-Score: {f1_baseline:.4f}, Улучшенный F1-Score: {f1_best:.4f}, Custom GB F1-Score: {f1_custom_gb:.4f}, Custom GB улучшенный F1-Score: {f1_custom_gb_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## Задача регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые 5 строк датасета для регрессии:\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "Используемые признаки: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
      "Размер X: (17379, 12)\n"
     ]
    }
   ],
   "source": [
    "path = 'hour.csv'\n",
    "data_reg = pd.read_csv(path)\n",
    "\n",
    "print(\"\\nПервые 5 строк датасета для регрессии:\")\n",
    "print(data_reg.head())\n",
    "\n",
    "# убрал дату в строковом представлении (есть дата в числовых колонках), так же число, которое предсказываем (cnt)\n",
    "useful_columns = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', \n",
    "                  'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "# Разделим данные на тестовую и обучающую выборку\n",
    "X = data_reg[useful_columns]\n",
    "y = data_reg['cnt']\n",
    "\n",
    "print(f\"\\nИспользуемые признаки: {list(X.columns)}\")\n",
    "print(f\"Размер X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделяем данны и масштабируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (13903, 12), Размер тестовой выборки: (3476, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}, Размер тестовой выборки: {X_test.shape}\")\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_scaled = scaler_reg.fit_transform(X_train)\n",
    "X_test_scaled = scaler_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение бейзлайна и оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Бейзлайн Gradient Boosting Regressor:\n",
      "MAE: 47.1515, MSE: 4766.6716, R²: 0.8495\n"
     ]
    }
   ],
   "source": [
    "gb_baseline = GradientBoostingRegressor(random_state=42)\n",
    "gb_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_reg = gb_baseline.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "print(\"\\nБейзлайн Gradient Boosting Regressor:\")\n",
    "mae_baseline, mse_baseline, r2_baseline = evaluate_model(y_test, y_pred_reg)\n",
    "print(f\"MAE: {mae_baseline:.4f}, MSE: {mse_baseline:.4f}, R²: {r2_baseline:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подберем гиперпараметры для улучшения модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42), \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"r2\", \n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params_reg = grid_search.best_params_\n",
    "print(f\"Лучшие параметры: {best_params_reg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модели с подобранными гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Улучшенный Gradient Boosting Regressor:\n",
      "MAE: 23.4889, MSE: 1543.0930, R²: 0.9513\n"
     ]
    }
   ],
   "source": [
    "gb_optimized = grid_search.best_estimator_\n",
    "gb_optimized.fit(X_train_scaled, y_train)\n",
    "y_pred_optimized = gb_optimized.predict(X_test_scaled)\n",
    "\n",
    "mae_optimized, mse_optimized, r2_optimized = evaluate_model(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"\\nУлучшенный Gradient Boosting Regressor:\")\n",
    "print(f\"MAE: {mae_optimized:.4f}, MSE: {mse_optimized:.4f}, R²: {r2_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем свой алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.models = []\n",
    "        self.base_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        residuals = y - self.base_prediction\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                random_state=42 + i\n",
    "            )\n",
    "            tree.fit(X, residuals)\n",
    "            predictions = tree.predict(X)\n",
    "\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на результаты кастомной реализации с параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Gradient Boosting Regressor (базовый):\n",
      "MAE: 47.1493, MSE: 4766.5562, R²: 0.8495\n",
      "\n",
      "Custom Gradient Boosting Regressor (улучшенный):\n",
      "MAE: 23.4550, MSE: 1543.3993, R²: 0.9513\n"
     ]
    }
   ],
   "source": [
    "custom_gb_reg = CustomGradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3\n",
    ")\n",
    "custom_gb_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg = custom_gb_reg.predict(X_test_scaled)\n",
    "\n",
    "mae_custom, mse_custom, r2_custom = evaluate_model(y_test, y_pred_custom_reg)\n",
    "\n",
    "print(\"\\nCustom Gradient Boosting Regressor (базовый):\")\n",
    "print(f\"MAE: {mae_custom:.4f}, MSE: {mse_custom:.4f}, R²: {r2_custom:.4f}\")\n",
    "\n",
    "custom_gb_reg_improved = CustomGradientBoostingRegressor(\n",
    "    n_estimators=best_params_reg['n_estimators'],\n",
    "    learning_rate=best_params_reg['learning_rate'],\n",
    "    max_depth=best_params_reg['max_depth'],\n",
    "    min_samples_split=2\n",
    ")\n",
    "custom_gb_reg_improved.fit(X_train_scaled, y_train)\n",
    "y_pred_custom_reg_improved = custom_gb_reg_improved.predict(X_test_scaled)\n",
    "\n",
    "mae_custom_improved, mse_custom_improved, r2_custom_improved = evaluate_model(y_test, y_pred_custom_reg_improved)\n",
    "\n",
    "print(\"\\nCustom Gradient Boosting Regressor (улучшенный):\")\n",
    "print(f\"MAE: {mae_custom_improved:.4f}, MSE: {mse_custom_improved:.4f}, R²: {r2_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на все результаты и сравним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение результатов для задачи регрессии:\n",
      "Бейзлайн MAE: 47.1515, Улучшенный MAE: 23.4889, Custom GB MAE: 47.1493, Custom GB улучшенный MAE: 23.4550\n",
      "Бейзлайн MSE: 4766.6716, Улучшенный MSE: 1543.0930, Custom GB MSE: 4766.5562, Custom GB улучшенный MSE: 1543.3993\n",
      "Бейзлайн R²: 0.8495, Улучшенный R²: 0.9513, Custom GB R²: 0.8495, Custom GB улучшенный R²: 0.9513\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСравнение результатов для задачи регрессии:\")\n",
    "print(f\"Бейзлайн MAE: {mae_baseline:.4f}, Улучшенный MAE: {mae_optimized:.4f}, Custom GB MAE: {mae_custom:.4f}, Custom GB улучшенный MAE: {mae_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн MSE: {mse_baseline:.4f}, Улучшенный MSE: {mse_optimized:.4f}, Custom GB MSE: {mse_custom:.4f}, Custom GB улучшенный MSE: {mse_custom_improved:.4f}\")\n",
    "print(f\"Бейзлайн R²: {r2_baseline:.4f}, Улучшенный R²: {r2_optimized:.4f}, Custom GB R²: {r2_custom:.4f}, Custom GB улучшенный R²: {r2_custom_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Подведем итог"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача классификации (Wine Quality Dataset)\n",
    "\n",
    "| Алгоритм | Бейзлайн Accuracy | Улучшенный Accuracy | Custom Accuracy | Улучшенный Custom Accuracy |\n",
    "|----------|-------------------|---------------------|-----------------|----------------------------|\n",
    "| **KNN** | 0.7312 | 0.7500 | 0.7312 | 0.7500 |\n",
    "| **Логистическая регрессия** | 0.7406 | 0.7375 | 0.7375 | 0.7406 |\n",
    "| **Решающее дерево** | 0.7562 | 0.7594 | 0.7188 | 0.7531 |\n",
    "| **Случайный лес** | 0.8063 | 0.8125 | 0.8000 | 0.7969 |\n",
    "| **Градиентный бустинг** | 0.7906 | **0.8281** | 0.7063 | 0.7250 \n",
    "\n",
    "\n",
    "### Задача регрессии (Bike Sharing Dataset)\n",
    "\n",
    "| Алгоритм | Бейзлайн R² | Улучшенный R² | Custom R² | Улучшенный Custom R² |\n",
    "|----------|-------------|---------------|-----------|----------------------|\n",
    "| **KNN** | 0.9527 | 0.9514 | 0.9494 | 0.9514 |\n",
    "| **Линейная регрессия** | **0.9666** | **0.9666** | **0.9666** | **0.9666** |\n",
    "| **Случайный лес** | 0.9441 | 0.9449 | 0.9439 | 0.9447 |\n",
    "| **Градиентный бустинг** | 0.8495 | 0.9513 | 0.8495 | 0.9513 |\n",
    "\n",
    "### Эффективность алгоритмов\n",
    "- **Градиентный бустинг** показал наилучшие результаты после настройки гиперпараметров\n",
    "- **Ансамблевые методы** (Random Forest, Gradient Boosting) в целом превосходят одиночные алгоритмы\n",
    "- **KNN** демонстрирует стабильную производительность на обоих типах задач\n",
    "\n",
    "### Влияние настройки гиперпараметров\n",
    "- Наибольший прирост от настройки: **Градиентный бустинг** (+0.0375 в классификации, +0.1018 в регрессии)\n",
    "- Наименьший прирост: **Линейная регрессия** (минимальные изменения)\n",
    "- **Custom реализации** в большинстве случаев уступают sklearn-версиям\n",
    "\n",
    "### Выводы\n",
    "\n",
    "1. **Для классификации** рекомендуется использовать **Gradient Boosting** с тщательной настройкой гиперпараметров\n",
    "2. **Для регрессии** лучшие результаты показали **Linear Regression** и **Gradient Boosting**\n",
    "3. **Custom реализации** в среднем на 2-10% уступают оптимизированным sklearn-версиям\n",
    "4. **Ансамблевые методы** требуют значительной настройки, но дают лучшие результаты\n",
    "5. **KNN** остается хорошим baseline-алгоритмом благодаря простоте и стабильности"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
